{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4T4wAZZkQz"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf4nvQUjkymN",
        "outputId": "315d233a-0e41-447d-f10b-7b462d92ceed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Importação de bibliotecas e configuração de caminhos\n",
        "import os\n",
        "import re\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pickle\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from packaging import version\n",
        "from IPython import display\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Caminhos dos dados\n",
        "SPECTRA_PATH = '../datasets/spectras/'\n",
        "DATASET_PATH = '../datasets'\n",
        "MODELS_PATH = '../models'\n",
        "HISTORY_PATH = '../history'\n",
        "\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4KvAqNDk7eq"
      },
      "outputs": [],
      "source": [
        "# Dicionário de grupos funcionais e seus SMARTS\n",
        "func_grp_smarts = {\n",
        "    'alkane':'[CX4;H0,H1,H2,H4]',\n",
        "    'methyl':'[CH3]',\n",
        "    'alkene':'[CX3]=[CX3]',\n",
        "    'alkyne':'[CX2]#C',\n",
        "    'alcohols':'[#6][OX2H]',\n",
        "    'amines':'[NX3;H2,H1;!$(NC=O)]',\n",
        "    'nitriles':'[NX1]#[CX2]',\n",
        "    'aromatics':'[$([cX3](:*):*),$([cX2+](:*):*)]',\n",
        "    'alkyl halides':'[#6][F,Cl,Br,I]',\n",
        "    'esters':'[#6][CX3](=O)[OX2H0][#6]',\n",
        "    'ketones':'[#6][CX3](=O)[#6]',\n",
        "    'aldehydes':'[CX3H1](=O)[#6]',\n",
        "    'carboxylic acids':'[CX3](=O)[OX2H1]',\n",
        "    'ether': '[OD2]([#6])[#6]',\n",
        "    'acyl halides':'[CX3](=[OX1])[F,Cl,Br,I]',\n",
        "    'amides':'[NX3][CX3](=[OX1])[#6]',\n",
        "    'nitro':'[$([NX3](=O)=O),$([NX3+](=O)[O-])][!#8]'}\n",
        "\n",
        "column_names = list(func_grp_smarts.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "vq5-hM26e_kn",
        "outputId": "f90bd36e-e158-4b83-d653-3555f16f438f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1030</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1030</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">528,390</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1030\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m527,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m3,225\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1030\u001b[0m)           │       \u001b[38;5;34m528,390\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,174,943</span> (15.93 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,174,943\u001b[0m (15.93 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,391,647</span> (5.31 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,391,647\u001b[0m (5.31 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,783,296</span> (10.62 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,783,296\u001b[0m (10.62 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Carregar autoencoder salvo\n",
        "\n",
        "autoencoder_load = keras.models.load_model(os.path.join(MODELS_PATH, 'autoencoder_model_2025_04_07.keras'))\n",
        "autoencoder_load.summary()\n",
        "encoder_model_load = keras.Model(inputs=autoencoder_load.input, outputs=autoencoder_load.layers[4].output)  # Encoder até a camada de codificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyLBKjbspRrB",
        "outputId": "849f8bef-0b15-4b25-f6c9-bcf8c35c4d2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8241"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Carregar autoencoder salvo\n",
        "from tensorflow import keras\n",
        "\n",
        "autoencoder_path = os.path.join(MODELS_PATH, 'autoencoder_model_2025_04_07.keras')\n",
        "autoencoder_load = keras.models.load_model(autoencoder_path)\n",
        "autoencoder_load.summary()\n",
        "encoder_model_load = keras.Model(inputs=autoencoder_load.input, outputs=autoencoder_load.layers[4].output)  # Encoder até a camada de codificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVmuGUTnqjvB",
        "outputId": "4592bf96-9f41-4da8-d3f0-0d847e9bd285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1030, 24636)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Carregar dataset de enriquecimento\n",
        "df_enrich_path = os.path.join(DATASET_PATH, 'df_enrich.csv')\n",
        "df_enrich = pd.read_csv(df_enrich_path)\n",
        "dataset_y = df_enrich.copy()\n",
        "dataset_y.index = dataset_y['CAS']\n",
        "print(f'Número de CAS únicos: {len(dataset_y.CAS.unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar espectros processados\n",
        "# df_spectra_all = pd.read_csv(os.path.join(DATASET_PATH, 'df_spectra_all_mixture_interpolate.csv'))\n",
        "df_spectra_path = os.path.join(DATASET_PATH, 'df_spectra_all_mixture_interpolate.parquet')\n",
        "df_spectra_all = pd.read_parquet(df_spectra_path)\n",
        "mean_cols = [x for x in df_spectra_all.columns if 'mean' in x]\n",
        "min_cols = [x for x in df_spectra_all.columns if 'min' in x]\n",
        "max_cols = [x for x in df_spectra_all.columns if 'max' in x]\n",
        "all_cols = mean_cols + min_cols + max_cols\n",
        "print(f'Shape do DataFrame de espectros: {df_spectra_all.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BuUcKt3BlCxX"
      },
      "outputs": [],
      "source": [
        "def load_history(file_path):\n",
        "  return pickle.load(open(f'{file_path}', 'rb'))\n",
        "\n",
        "\n",
        "def get_dataset(df_spectra_all, dataset_y, agg_func):\n",
        "\n",
        "  if agg_func == 'mean':\n",
        "    dataset_x = df_spectra_all[mean_cols].copy()\n",
        "  elif agg_func == 'min':\n",
        "    dataset_x = df_spectra_all[min_cols].copy()\n",
        "\n",
        "  elif agg_func == 'max':\n",
        "    dataset_x = df_spectra_all[max_cols].copy()\n",
        "\n",
        "  # dataset_x = df_spectra_all.copy()\n",
        "  dataset_x = dataset_x.T\n",
        "  dataset_x.columns = ['bin_' + str(x) for x in dataset_x.columns]\n",
        "  dataset_x.reset_index(inplace=True)\n",
        "  dataset_x.index = dataset_x['index'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "  dataset_y = dataset_y[dataset_y['yunits'] == 'ABSORBANCE']\n",
        "\n",
        "  dataset_final = pd.merge(dataset_y, dataset_x, left_index = True, right_index = True, how='inner')\n",
        "\n",
        "  return dataset_final\n",
        "\n",
        "def find_best_epoch(history):\n",
        "    \"\"\"\n",
        "    Finds the epoch with the lowest validation loss.\n",
        "\n",
        "    Args:\n",
        "        history: Training history object from Keras model.fit().\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the best epoch number and its corresponding validation loss.\n",
        "        Returns None if history object is invalid or empty.\n",
        "    \"\"\"\n",
        "    if not history or 'val_loss' not in history.history:\n",
        "        return None\n",
        "\n",
        "    val_losses = history.history['val_loss']\n",
        "    best_epoch = np.argmin(val_losses)  # Index of the minimum validation loss\n",
        "    best_val_loss = val_losses[best_epoch]\n",
        "\n",
        "    return best_epoch, best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10Nsv9ek1N7"
      },
      "outputs": [],
      "source": [
        "def load_history(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def get_dataset(df_spectra_all, dataset_y, agg_func):\n",
        "    if agg_func == 'mean':\n",
        "        dataset_x = df_spectra_all[mean_cols].copy()\n",
        "    elif agg_func == 'min':\n",
        "        dataset_x = df_spectra_all[min_cols].copy()\n",
        "    elif agg_func == 'max':\n",
        "        dataset_x = df_spectra_all[max_cols].copy()\n",
        "    else:\n",
        "        raise ValueError(f\"agg_func '{agg_func}' não reconhecido. Use 'mean', 'min' ou 'max'.\")\n",
        "\n",
        "    dataset_x = dataset_x.T\n",
        "    dataset_x.columns = [f'bin_{x}' for x in dataset_x.columns]\n",
        "    dataset_x.reset_index(inplace=True)\n",
        "    dataset_x.index = dataset_x['index'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "    dataset_y = dataset_y[dataset_y['yunits'] == 'ABSORBANCE']\n",
        "\n",
        "    dataset_final = pd.merge(dataset_y, dataset_x, left_index=True, right_index=True, how='inner')\n",
        "    return dataset_final\n",
        "\n",
        "def find_best_epoch(history):\n",
        "    \"\"\"\n",
        "    Encontra a época com menor loss de validação.\n",
        "    Args:\n",
        "        history: Objeto de histórico do treinamento do Keras.\n",
        "    Returns:\n",
        "        Uma tupla (best_epoch, best_val_loss) ou None.\n",
        "    \"\"\"\n",
        "    if not history or 'val_loss' not in history.history:\n",
        "        return None\n",
        "    val_losses = history.history['val_loss']\n",
        "    best_epoch = np.argmin(val_losses)\n",
        "    best_val_loss = val_losses[best_epoch]\n",
        "    return best_epoch, best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNvaDuWpC5q-",
        "outputId": "db1bc291-5ed2-4dcc-8b50-0a3fdcec4cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (6157, 1030)\n",
            "Shape of X_validation: (1129, 1030)\n",
            "Shape of X_test: (924, 1030)\n",
            "Shape of Y_train: (6157, 17)\n",
            "Shape of Y_validation: (1129, 17)\n",
            "Shape of Y_test: (924, 17)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_validation:\", X_validation.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)\n",
        "print(\"Shape of Y_validation:\", Y_validation.shape)\n",
        "print(\"Shape of Y_test:\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cFTlQpMwZV0F"
      },
      "outputs": [],
      "source": [
        "def compute_model_analysis(agg_func, data_prep, current_date, callbacks=False, save_history=True):\n",
        "  dataset_final = get_dataset(df_spectra_all, dataset_y, agg_func)\n",
        "\n",
        "  X = dataset_final[[col for col in dataset_final.columns if 'bin' in col]]\n",
        "  X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "  if data_prep == 'encoder':\n",
        "    X = encoder_model_load.predict(X)\n",
        "\n",
        "  Y = dataset_final[column_names].apply(lambda x: x.astype(int))\n",
        "  # X_train, X_test_temp, Y_train, Y_test_temp = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # X_validation, X_test, Y_validation, Y_test = train_test_split(X_test_temp, Y_test_temp, test_size=0.45, random_state=42)\n",
        "\n",
        "  X_train, X_test_temp, Y_train, Y_test_temp = train_test_split(X, Y, test_size=0.15, random_state=42,stratify = Y[column_names].sum(axis=1))\n",
        "\n",
        "  X_validation, X_test, Y_validation, Y_test = train_test_split(X_test_temp, Y_test_temp, test_size=0.30, random_state=42 , stratify = Y_test_temp[column_names].sum(axis=1))\n",
        "\n",
        "\n",
        "  tf.random.set_seed(42)\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      layers.Input(shape=(X_train.shape[1],)),\n",
        "      layers.Dense(255, activation='relu'),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(215, activation='relu'),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(165, activation='relu'),\n",
        "      layers.Dense(Y_train.shape[1],activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy','f1_score','binary_accuracy','precision', 'recall', 'binary_crossentropy'])\n",
        "\n",
        "  #Define the callbacks\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', start_from_epoch = 30, restore_best_weights=True)\n",
        "  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, min_lr=1e-4, verbose=0, mode='min')\n",
        "  # mcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "  if callbacks:\n",
        "    callbacks_list = [early_stopping, reduce_lr]\n",
        "  else:\n",
        "    callbacks_list = []\n",
        "\n",
        "  history = model.fit(X_train, Y_train,\n",
        "            validation_data=(X_validation, Y_validation),\n",
        "            callbacks=callbacks_list,\n",
        "            epochs=100, verbose=1,shuffle = True)\n",
        "\n",
        "  # Evaluate the model on the validation set\n",
        "  val_loss = history.history['val_loss'][-1] # Get the last validation loss\n",
        "\n",
        "  # Evaluate the model\n",
        "  y_pred = model.predict(X_validation)\n",
        "  y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "  compose_name = f'mlp_{agg_func}_{data_prep}_{current_date}_call_{callbacks}'\n",
        "  model.save(f'{compose_name}.keras')\n",
        "\n",
        "  if save_history:\n",
        "    # Save the history object\n",
        "    with open(f'{compose_name}_history.pkl', 'wb') as f:\n",
        "      pickle.dump(history.history, f)\n",
        "\n",
        "\n",
        "  best_epoch, best_val_loss = find_best_epoch(history)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16, 10))\n",
        "  plt.suptitle(f'Métricas Treinamento - Época Escolhida {best_epoch} - {best_val_loss:.3f} Validação Loss Function')\n",
        "\n",
        "\n",
        "  plt.subplot(3, 1, 1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "  plt.ylabel('Acurácia')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "\n",
        "  plt.subplot(3, 1, 2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "\n",
        "\n",
        "\n",
        "  plt.subplot(3, 1, 3)\n",
        "  plt.plot(history.history['binary_accuracy'])\n",
        "  plt.plot(history.history['val_binary_accuracy'])\n",
        "  plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "  plt.ylabel('Acurácia Binarizada')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "  plt.savefig(f'{compose_name}.png')\n",
        "  plt.show()\n",
        "\n",
        "  return history\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKilf6RBhrgu",
        "outputId": "4cf8ca20-10c0-47b0-d6e7-7c474c4ec7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        }
      ],
      "source": [
        "callbacks = True\n",
        "mlp_min_normal_history = compute_model_analysis('min', 'normal', current_date, callbacks=callbacks)\n",
        "mlp_min_encoder_history = compute_model_analysis('min', 'encoder', current_date, callbacks=callbacks)\n",
        "mlp_max_normal_history = compute_model_analysis('max', 'normal', current_date,  callbacks=callbacks)\n",
        "mlp_max_encoder_history = compute_model_analysis('max', 'encoder', current_date,  callbacks=callbacks)\n",
        "mlp_mean_normal_history = compute_model_analysis('mean', 'normal', current_date,  callbacks=callbacks)\n",
        "mlp_mean_encoder_history = compute_model_analysis('mean', 'encoder', current_date,  callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYWkMzWehlny"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Plot for Min function\n",
        "plt.plot(mlp_min_encoder_history.history['loss'], label='Min Auto encoder - Train', color='green', marker='o', linestyle='dashed')\n",
        "plt.plot(mlp_min_encoder_history.history['val_loss'], label='Min Auto - Validation', color='green', linestyle='solid')\n",
        "\n",
        "# Plot for Min function\n",
        "plt.plot(mlp_min_normal_history.history['loss'], label='Min Normal encoder - Train', color='green', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_min_normal_history.history['val_loss'], label='Min Normal - Validation', color='green', linestyle='solid')\n",
        "\n",
        "\n",
        "# Plot for Max function\n",
        "plt.plot(mlp_max_encoder_history.history['loss'], label='Max - Train', color='blue', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_max_encoder_history.history['val_loss'], label='Max - Validation', color='blue', linestyle='solid')\n",
        "\n",
        "# Plot for Mean function\n",
        "plt.plot(mlp_mean_encoder_history.history['loss'], label='Mean - Train', color='red', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_mean_encoder_history.history['val_loss'], label='Mean - Validation', color='red', linestyle='solid')\n",
        "\n",
        "\n",
        "# plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "plt.ylabel('Função de Custo')\n",
        "plt.xlabel('Época')\n",
        "plt.legend()\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE2bicHB2A-T"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Plot for Min function\n",
        "plt.plot(mlp_min_encoder_history.history['loss'], label='Min - Train', color='green', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_min_encoder_history.history['val_loss'], label='Min - Validation', color='green')\n",
        "\n",
        "# Plot for Max function\n",
        "plt.plot(mlp_max_encoder_history.history['loss'], label='Max - Train', color='blue', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_max_encoder_history.history['val_loss'], label='Max - Validation', color='blue')\n",
        "\n",
        "# Plot for Mean function\n",
        "plt.plot(mlp_mean_encoder_history.history['loss'], label='Mean - Train', color='red', marker='', linestyle='dashed')\n",
        "plt.plot(mlp_mean_encoder_history.history['val_loss'], label='Mean - Validation', color='red')\n",
        "\n",
        "\n",
        "# plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "plt.ylabel('Função de Custo')\n",
        "plt.xlabel('Época')\n",
        "plt.legend()\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZrfnjHay3xR"
      },
      "outputs": [],
      "source": [
        "mlp_min_encoder_history\n",
        "mlp_mean_encoder_history\n",
        "mlp_max_encoder_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGRjwK7PzF_c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "plt.suptitle(f'Métricas Treinamento - Época Escolhida {best_epoch} - {best_val_loss:.3f} Validação Loss Function')\n",
        "\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(history.history['binary_accuracy'])\n",
        "plt.plot(history.history['val_binary_accuracy'])\n",
        "plt.axvline(x=best_epoch, color='red', linestyle='--')\n",
        "plt.ylabel('Acurácia Binarizada')\n",
        "plt.xlabel('Epoch')\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:.3f}\".format(x)))\n",
        "plt.savefig(f'{compose_name}.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMRPXUfFmDi2"
      },
      "source": [
        "# Análises Finais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apFsVmqelEXD"
      },
      "outputs": [],
      "source": [
        "def compute_general_metrics(y_real,y_pred,i=None):\n",
        "\n",
        "  if i is None:\n",
        "    names = 'general'\n",
        "    accur = metrics.accuracy_score(y_real, y_pred)\n",
        "    f1 = metrics.f1_score(y_real, y_pred, average='weighted')\n",
        "    prec = metrics.precision_score(y_real, y_pred, average='weighted')\n",
        "    rec = metrics.recall_score(y_real, y_pred, average='weighted')\n",
        "    jacc = float(metrics.jaccard_score(y_real, y_pred, average='weighted'))\n",
        "    hamm = metrics.hamming_loss(y_real, y_pred)\n",
        "    log_loss = metrics.log_loss(y_real, y_pred)\n",
        "\n",
        "  elif i is not None:\n",
        "    names = column_names[i]\n",
        "    accur = metrics.accuracy_score(y_real.iloc[:, i], y_pred[:, i])\n",
        "    f1 = metrics.f1_score(y_real.iloc[:, i], y_pred[:, i],zero_division=0)\n",
        "    prec = metrics.precision_score(y_real.iloc[:, i], y_pred[:, i],zero_division=0)\n",
        "    rec = metrics.recall_score(y_real.iloc[:, i], y_pred[:, i],zero_division=0)\n",
        "    jacc = float(metrics.jaccard_score(y_real.iloc[:, i], y_pred[:, i]))\n",
        "    hamm = metrics.hamming_loss(y_real.iloc[:, i], y_pred[:, i])\n",
        "    log_loss = metrics.log_loss(y_real.iloc[:, i], y_pred[:, i])\n",
        "\n",
        "  return [names, accur, f1, prec, rec, jacc, hamm, log_loss]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2c3wpFknbh"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def compute_all_comparisons(func_model,data_model):\n",
        "  model_prefix = f'mlp_{func_model}_{data_model}'\n",
        "  model_suffix = f'_2025_08_18_call_True'\n",
        "  model_name = model_prefix + model_suffix\n",
        "  model = keras.models.load_model(f'/content/{model_name}.keras')\n",
        "\n",
        "  dataset_final = get_dataset(df_spectra_all, dataset_y, func_model)\n",
        "\n",
        "  X = dataset_final[[col for col in dataset_final.columns if 'bin' in col]]\n",
        "  X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "  Y = dataset_final[column_names].apply(lambda x: x.astype(int))\n",
        "  X_train, X_test_temp, Y_train, Y_test_temp = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "\n",
        "  X_validation, X_test, Y_validation, Y_test = train_test_split(X_test_temp, Y_test_temp, test_size=0.45, random_state=42)\n",
        "\n",
        "  if data_model == 'encoder':\n",
        "    X_train = pd.DataFrame(encoder_model_load.predict(X_train))\n",
        "    X_test = pd.DataFrame(encoder_model_load.predict(X_test))\n",
        "    X_test_temp = pd.DataFrame(encoder_model_load.predict(X_test_temp))\n",
        "\n",
        "  train_predict = (model.predict(X_train) > 0.5).astype(int)\n",
        "  test_predict = (model.predict(X_test_temp) > 0.5).astype(int)\n",
        "\n",
        "  metrics_list = ['accuracy','f1_score','precision', 'recall', 'jaccard','hamming','log_loss']\n",
        "\n",
        "  train_metrics = [compute_general_metrics(Y_train,train_predict)] + [compute_general_metrics(Y_train,train_predict,x) for x in range(len(column_names))]\n",
        "  test_metrics = [compute_general_metrics(Y_test_temp,test_predict)] + [compute_general_metrics(Y_test_temp,test_predict,x) for x in range(len(column_names))]\n",
        "\n",
        "  full_metrics_df = pd.DataFrame(train_metrics)\n",
        "  full_metrics_df.columns = ['metric'] + metrics_list\n",
        "  full_metrics_df['data'] = 'train'\n",
        "\n",
        "  test_metrics_df = pd.DataFrame(test_metrics)\n",
        "  test_metrics_df.columns = ['metric'] + metrics_list\n",
        "  test_metrics_df['data'] = 'test'\n",
        "\n",
        "  merged_df = pd.concat([full_metrics_df, test_metrics_df], ignore_index=True)\n",
        "  merged_df['model'] = model_prefix\n",
        "\n",
        "  return merged_df\n",
        "\n",
        "results_list = []\n",
        "for func_model in ['min','max','mean']:\n",
        "  for data_model in ['encoder','normal']:\n",
        "    results_list.append(compute_all_comparisons(func_model,data_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M0QmrU3ZJ8Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5DfdrGTzoo"
      },
      "outputs": [],
      "source": [
        "all_metrics = pd.concat(results_list)\n",
        "# all_metrics[(all_metrics['model'] == 'mlp_min_normal') & (all_metrics['data'] == 'test')][['metric','accuracy','f1_score','hamming','data']]\n",
        "all_metrics[(all_metrics['data'] == 'test')].sort_values(by=['hamming','model'], ascending = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StprVRVzY_Aa"
      },
      "outputs": [],
      "source": [
        "all_metrics = pd.concat(results_list)\n",
        "# .groupby(['model','data','metric'])\n",
        "all_metrics.to_csv('all_metrics_mlp.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ush7QktWmCKC"
      },
      "outputs": [],
      "source": [
        "all_metrics.groupby(['model','data','metric']).sum().to_csv('all_metrics_mlp_agg.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
